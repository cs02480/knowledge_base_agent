# src/llm_integrator.py

import os
import logging
from typing import List, Dict, Any, Optional

# For Ollama Integration
from ollama import Client as OllamaClient # Renamed for clarity

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class LLMIntegrator:
    """
    Manages interaction with a Large Language Model (LLM).
    Configured for Ollama provider.
    """
    def __init__(self, llm_provider: str = "ollama", model_name: str = "qwen:8b"): # Default to ollama, qwen:7b as common example
        self.llm_provider = llm_provider.lower()
        self.model_name = model_name
        self.client = None
        
        if self.llm_provider == "ollama":
            # Ollama client initialization
            # Ensure your Ollama server is running (default host is http://localhost:11434)
            ollama_host = 'http://localhost:11434'
            self.client = OllamaClient(host=ollama_host) 
            logging.info(f"LLMIntegrator initialized for Ollama with model: {model_name} on {ollama_host}")
            
            # Optional: Test if the model is available
            try:
                self.client = OllamaClient(host=ollama_host)
                # This is the line that was causing the error. It must now use 'ollama_host'
                logging.info(f"LLMIntegrator initialized for Ollama with model: {model_name} on {ollama_host}") 

                self.client.list() 
                logging.info(f"Ollama server connected. Model '{self.model_name}' expected to be available.")
            except Exception as e:
                logging.error(f"Could not connect to Ollama server or verify model '{self.model_name}': {e}. "
                              f"Please ensure Ollama server is running at {ollama_host} " 
                              f"and the model '{self.model_name}' is pulled (e.g., 'ollama pull {self.model_name}').", exc_info=True)
                raise ConnectionError(f"Failed to initialize Ollama client or connect to server at {ollama_host}.") from e
        else:
            logging.error(f"Unsupported LLM provider: {llm_provider}. This LLMIntegrator is configured for 'ollama'.")
            raise ValueError(f"Unsupported LLM provider: {llm_provider}.")

    def generate_response(self, prompt: str, temperature: float = 0.7) -> Optional[str]:
        """
        Generates a response from the LLM based on the given prompt.
        """
        if self.client is None:
            logging.error("LLM client not initialized. Cannot generate response.")
            return None

        try:
            if self.llm_provider == "ollama":
                response = self.client.chat(
                    model=self.model_name,
                    messages=[
                        {'role': 'system', 'content': "You are a helpful AI assistant specialized in providing concise and informative answers based on provided context. If the answer is not available in the context, clearly state that you don't have enough information."},
                        {'role': 'user', 'content': prompt}
                    ],
                    options={'temperature': temperature} # Ollama uses 'options' for temperature
                )
                generated_text = response['message']['content']
                logging.info(f"Response generated by Ollama model '{self.model_name}'.")
                return generated_text
            else:
                return f"No generation logic for provider: {self.llm_provider}"
        except Exception as e:
            logging.error(f"Error generating response from LLM ({self.llm_provider}): {e}", exc_info=True)
            return "An error occurred while generating a response."